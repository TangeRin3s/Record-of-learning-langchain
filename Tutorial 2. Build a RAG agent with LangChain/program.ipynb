{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4f33ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (0.4.1)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.2 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain) (1.0.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (0.4.59)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-community) (2.3.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain) (2.6.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\15509\\anaconda3\\envs\\langchain\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.14.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>=1.6.1 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.8.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.14.3-py3-none-any.whl (107 kB)\n",
      "Downloading soupsieve-2.8.1-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "\n",
      "   ------------- -------------------------- 1/3 [beautifulsoup4]\n",
      "   ---------------------------------------- 3/3 [bs4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.14.3 bs4-0.0.2 soupsieve-2.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-text-splitters langchain-community bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7c7514",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddf1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9b542f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c61674",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca7171e",
   "metadata": {},
   "source": [
    "### Load Documents from an URl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18333076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 43047\n"
     ]
    }
   ],
   "source": [
    "# bs4ï¼šBeautifulSoupï¼Œç”¨äºè§£æ HTML\n",
    "# WebBaseLoaderï¼šLangChain æä¾›çš„ç½‘é¡µåŠ è½½å™¨ï¼Œç”¨æ¥æŠ“å–ç½‘é¡µå¹¶è½¬æˆæ–‡æ¡£å¯¹è±¡\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "headers = {\n",
    "    \"User-Agent\": \"SpagBolOntology/0.1 (charlie; GuanSc@proton.com)\"\n",
    "}\n",
    "\n",
    "# SoupStrainer çš„ä½œç”¨ï¼š\n",
    "# ğŸ‘‰ åªè§£æ HTML ä¸­ç¬¦åˆæ¡ä»¶çš„éƒ¨åˆ†\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "# å‘Šè¯‰ loaderï¼š\n",
    "# ğŸ‘‰ ç”¨ BeautifulSoup\n",
    "# ğŸ‘‰ ä¸”åªè§£æ bs4_strainer æŒ‡å®šçš„å†…å®¹\n",
    "# web_paths æ˜¯ tupleï¼ˆå³ä½¿åªæœ‰ä¸€ä¸ª URL ä¹Ÿè¦åŠ é€—å·ï¼‰\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    "    header_template={\n",
    "        \"User-Agent\": \"Rag-test/0.1 (charlie; ucabs95@ucl.ac.uk)\"\n",
    "    },\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6768a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99c8cf",
   "metadata": {},
   "source": [
    "### Splitting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34c34b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ade8f",
   "metadata": {},
   "source": [
    "### Storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03b969c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1f40c92a-de4d-45d9-bc18-cd43d35f182a', 'a8cebb83-73f5-4058-a1ff-d3295590395d', '29a94d89-0b54-4276-9430-bcedf17e7fe8']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850a9270",
   "metadata": {},
   "source": [
    "# 2. Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8eb2ef",
   "metadata": {},
   "source": [
    "### RAG agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d9634",
   "metadata": {},
   "source": [
    "response_format=\"content_and_artifact\" æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "è¡¨ç¤ºè¿™ä¸ª tool è¿”å›ä¸¤ä¸ªéƒ¨åˆ†ï¼š\n",
    "| è¿”å›éƒ¨åˆ†       | å«ä¹‰              |\n",
    "| ---------- | --------------- |\n",
    "| `content`  | ç»™ LLM ç”¨æ¥â€œè¯»â€çš„æ–‡æœ¬  |\n",
    "| `artifact` | ç»™ç³»ç»Ÿ/ç¨‹åºç»§ç»­å¤„ç†çš„åŸå§‹å¯¹è±¡ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea36203",
   "metadata": {},
   "source": [
    "### Build a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45caa645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        # æ¯ä¸ªæ–‡æ¡£è¢«æ ¼å¼åŒ–ä¸ºï¼š\n",
    "        # Source: {å…ƒæ•°æ®}\n",
    "        # Content: {æ­£æ–‡}\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    # | è¿”å›å€¼              | ç”¨é€”              |\n",
    "    # | ---------------- | --------------- |\n",
    "    # | `serialized`     | ğŸ‘‰ ç»™ LLM å½“â€œä¸Šä¸‹æ–‡â€ |\n",
    "    # | `retrieved_docs` | ğŸ‘‰ ç»™ç¨‹åºåå¤„ç†       |\n",
    "\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6cd681",
   "metadata": {},
   "source": [
    "### Assemble an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c78f8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8748bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41de098",
   "metadata": {},
   "source": [
    "### ç”¨ LangGraph/LangChain çš„â€œçŠ¶æ€æœºå¼ Agentâ€åš æµå¼æ‰§è¡Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c609c",
   "metadata": {},
   "source": [
    "### 1ï¼‰agent.stream(...) æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "agent.stream(...) **ä¸æ˜¯ä¸€æ¬¡æ€§ç®—å®Œç»™ä½ ä¸€ä¸ªç»“æœ**ï¼Œè€Œæ˜¯è¿”å›ä¸€ä¸ªâ€œå¯è¿­ä»£å¯¹è±¡ / ç”Ÿæˆå™¨ï¼ˆgeneratorï¼‰â€ã€‚\n",
    "\n",
    "- ä½ å¯ä»¥æŠŠ agent æƒ³æˆä¸€ä¸ªä¼šç»å†å¤šæ­¥çš„æµç¨‹ï¼š\n",
    "  â€œè¯»è¾“å…¥ â†’ æ¨ç† â†’ï¼ˆå¯èƒ½ï¼‰è°ƒç”¨å·¥å…· â†’ å¾—åˆ°å·¥å…·ç»“æœ â†’ å†æ¨ç† â†’ è¾“å‡ºå›ç­”â€\n",
    "\n",
    "- **æ¯èµ°ä¸€æ­¥ï¼Œå®ƒå°± yield ä¸€ä¸ª event ç»™ä½ **\n",
    "\n",
    "- æ‰€ä»¥ä½ å¯ä»¥åœ¨ for event in ... é‡Œå®æ—¶çœ‹åˆ°å®ƒçš„è¿›å±•\n",
    "\n",
    "### 2ï¼‰for event in ... ä¸ºä»€ä¹ˆèƒ½å·¥ä½œï¼Ÿ\n",
    "\n",
    "å› ä¸º agent.stream(...) è¿”å›çš„ä¸œè¥¿ç±»ä¼¼äºï¼š\n",
    "```python\n",
    "def stream(...):\n",
    "    yield event1\n",
    "    yield event2\n",
    "    yield event3\n",
    "    ...\n",
    "```\n",
    "\n",
    "äºæ˜¯ Python çš„ for å¾ªç¯å°±ä¼šï¼š\n",
    "\n",
    "- æ‹¿åˆ° event1\n",
    "\n",
    "- æ‰§è¡Œå¾ªç¯ä½“ï¼ˆä½ åé¢æ‰“å° messageï¼‰\n",
    "\n",
    "- å†æ‹¿ event2\n",
    "\n",
    "â€¦â€¦ç›´åˆ° stream ç»“æŸ\n",
    "\n",
    "è¿™å°±æ˜¯ â€œstreamingâ€ çš„å«ä¹‰ï¼š**ä¸æ˜¯ç­‰æœ€åä¸€æ­¥ï¼Œè€Œæ˜¯è¾¹åšè¾¹åå‡ºä¸­é—´çŠ¶æ€ã€‚**\n",
    "\n",
    "### 3ï¼‰ä¼ å…¥çš„ç¬¬ä¸€ä¸ªå‚æ•°ï¼š{\"messages\": [...]} åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "```python\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": query}]}\n",
    "```\n",
    "\n",
    "è¿™æ˜¯ç»™ agent çš„åˆå§‹çŠ¶æ€ï¼ˆinitial stateï¼‰ã€‚\n",
    "\n",
    "#### 3.1 ä¸ºä»€ä¹ˆå« messagesï¼Ÿ\n",
    "\n",
    "åœ¨ LangChain / LangGraph é‡Œï¼Œå¾ˆå¤š agent éƒ½ç”¨ä¸€ä¸ªâ€œçŠ¶æ€å­—å…¸ stateâ€æ¥é©±åŠ¨æ‰§è¡Œï¼Œæœ€æ ¸å¿ƒçš„å­—æ®µé€šå¸¸å°±æ˜¯ï¼š\n",
    "\n",
    "- messages: ç›®å‰å¯¹è¯é‡Œæ‰€æœ‰æ¶ˆæ¯çš„åˆ—è¡¨ï¼ˆèŠå¤©å†å²ï¼‰\n",
    "\n",
    "- agent çš„æ¯ä¸€æ­¥ï¼Œä¼šåŸºäº messages ç»§ç»­è¿½åŠ æ–°çš„æ¶ˆæ¯ï¼ˆAI å›å¤ã€å·¥å…·è¿”å›ç­‰ï¼‰\n",
    "\n",
    "#### 3.2 messages ä¸ºä»€ä¹ˆæ˜¯ listï¼Ÿ\n",
    "\n",
    "å› ä¸ºå¯¹è¯æ˜¯å¤šè½®çš„ï¼Œå¿…é¡»èƒ½è¡¨ç¤ºï¼š\n",
    "\n",
    "- ç¬¬ 1 è½® user\n",
    "\n",
    "- ç¬¬ 1 è½® assistant\n",
    "\n",
    "-ç¬¬ 2 è½® user\n",
    "\n",
    "- tool message\n",
    "\n",
    "â€¦\n",
    "\n",
    "æ‰€ä»¥ç”¨åˆ—è¡¨æŒ‰æ—¶é—´é¡ºåºå­˜ã€‚\n",
    "\n",
    "#### 3.3 {\"role\": \"user\", \"content\": query} æ˜¯ä»€ä¹ˆæ ¼å¼ï¼Ÿ\n",
    "\n",
    "è¿™å°±æ˜¯å…¸å‹çš„ ChatML / OpenAI chat æ ¼å¼çš„ç®€åŒ–ç‰ˆï¼š\n",
    "\n",
    "- role=\"user\"ï¼šè¿™æ¡æ¶ˆæ¯æ¥è‡ªç”¨æˆ·\n",
    "\n",
    "- content=queryï¼šæ¶ˆæ¯å†…å®¹å°±æ˜¯ä½ ä¸Šé¢æ‹¼å‡ºæ¥çš„é‚£æ®µå­—ç¬¦ä¸²\n",
    "\n",
    "æ¢å¥è¯è¯´ï¼Œè¿™ä¸€å¨çš„å«ä¹‰å°±æ˜¯ï¼š\n",
    "\n",
    "**â€œæŠŠç”¨æˆ·çš„æé—® query ä½œä¸ºå¯¹è¯çš„ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼Œäº¤ç»™ agentã€‚â€**\n",
    "\n",
    "### 4ï¼‰stream_mode=\"values\" åˆ°åº•æ§åˆ¶äº†ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "stream_mode å†³å®šäº† stream è¿‡ç¨‹ä¸­ yield å‡ºæ¥çš„ event é•¿ä»€ä¹ˆæ ·ã€‚\n",
    "\n",
    "ä½ è®¾æˆï¼š\n",
    "```python\n",
    "stream_mode=\"values\"\n",
    "```\n",
    "\n",
    "ä¸€èˆ¬è¡¨ç¤ºï¼š\n",
    "\n",
    "æ¯æ¬¡ yield çš„ event éƒ½ç»™ä½ â€œå½“å‰å®Œæ•´ state çš„å€¼â€ã€‚\n",
    "\n",
    "ä¹Ÿå°±æ˜¯ event å¾€å¾€åƒè¿™æ ·ï¼ˆæ¦‚å¿µç¤ºæ„ï¼‰ï¼š\n",
    "\n",
    "```python\n",
    "event = {\n",
    "  \"messages\": [\n",
    "    {\"role\": \"user\", \"content\": \"...\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "    {\"role\": \"tool\", \"content\": \"...\"},\n",
    "    ...\n",
    "  ],\n",
    "  # å¯èƒ½è¿˜æœ‰åˆ«çš„ state å­—æ®µ\n",
    "}\n",
    "```\n",
    "#### 4.1 ä¸ºä»€ä¹ˆè¿™å¾ˆæœ‰ç”¨ï¼Ÿ\n",
    "\n",
    "å› ä¸ºä½ å¯ä»¥éšæ—¶ä» event[\"messages\"] çœ‹åˆ°ç›®å‰ä¸ºæ­¢æ•´ä¸ªå¯¹è¯çŠ¶æ€ï¼ˆåŒ…å«æ–°è¿½åŠ çš„ä¸œè¥¿ï¼‰ã€‚\n",
    "\n",
    "#### 4.2 å¦‚æœä¸æ˜¯ values ä¼šæ€æ ·ï¼Ÿ\n",
    "\n",
    "ä¸åŒæ¡†æ¶å®ç°ç•¥æœ‰å·®å¼‚ï¼Œä½†æ¦‚å¿µä¸Šï¼š\n",
    "\n",
    "- valuesï¼šç»™â€œå…¨é‡çŠ¶æ€â€\n",
    "\n",
    "- updatesï¼šåªç»™â€œè¿™ä¸€æ­¥æ–°å˜åŒ–çš„é‚£éƒ¨åˆ†â€ï¼ˆå¢é‡ï¼‰\n",
    "\n",
    "- debugï¼šç»™æ›´å¤šå†…éƒ¨æ‰§è¡Œä¿¡æ¯ï¼ˆèŠ‚ç‚¹å/è·¯ç”±/è€—æ—¶ç­‰ï¼‰\n",
    "\n",
    "ç”¨ values çš„å¥½å¤„æ˜¯ï¼šæœ€ç›´è§‚ã€æœ€ä¸å®¹æ˜“æ¼ä¿¡æ¯ï¼Œä»£ä»·æ˜¯ event å¯èƒ½æ›´å¤§ã€‚\n",
    "\n",
    "### 5ï¼‰è¿™æ®µä»£ç åœ¨æ‰§è¡Œæ—¶ä¼šç»å†ä»€ä¹ˆâ€œæ­¥éª¤â€ï¼Ÿ\n",
    "\n",
    "å‡è®¾ agent æ˜¯ä¸€ä¸ªä¼šç”¨æœç´¢å·¥å…·çš„ agentï¼Œå®ƒå¯èƒ½äº§ç”Ÿä¸‹é¢è¿™äº› eventï¼ˆç¤ºæ„ï¼‰ï¼š\n",
    "\n",
    "- ç¬¬ä¸€æ¬¡ yieldï¼šåˆšæŠŠ user message æ”¾è¿›å»\n",
    "\n",
    " event[\"messages\"] = [HumanMessage(query)]\n",
    "\n",
    "- ç¬¬äºŒæ¬¡ yieldï¼šassistant å…ˆç»™ä¸€ä¸ªä¸­é—´æƒ³æ³•/è®¡åˆ’ï¼ˆæˆ–ç›´æ¥å¼€å§‹å›ç­”ï¼‰\n",
    "\n",
    " event[\"messages\"] æœ«å°¾å¤šäº† AIMessage(...)\n",
    "\n",
    "- ç¬¬ä¸‰æ¬¡ yieldï¼šassistant å†³å®šè°ƒç”¨å·¥å…·ï¼ˆæ¯”å¦‚ searchï¼‰\n",
    "\n",
    " æœ«å°¾å¤šäº†ä¸€ä¸ª ToolCall æˆ–â€œè¦è°ƒç”¨å·¥å…·â€çš„æ¶ˆæ¯\n",
    "\n",
    "- ç¬¬å››æ¬¡ yieldï¼šå·¥å…·è¿”å›ç»“æœ\n",
    "\n",
    " æœ«å°¾å¤šäº† ToolMessage(...)\n",
    " \n",
    "- ç¬¬äº”æ¬¡ yieldï¼šassistant ç»¼åˆå·¥å…·ç»“æœï¼Œè¾“å‡ºæœ€ç»ˆå›ç­”\n",
    "\n",
    " æœ«å°¾å¤šäº†æœ€ç»ˆ AIMessage(...)\n",
    "\n",
    "ä½ ä¹‹æ‰€ä»¥ç”¨ streamï¼Œå°±æ˜¯ä¸ºäº†èƒ½çœ‹åˆ°è¿™äº›ä¸­é—´é˜¶æ®µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a88bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the standard method for Task Decomposition?\n",
      "\n",
      "Once you get the answer, look up common extensions of that method.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_9X3chempPJwwuNs0hVeKMqe0)\n",
      " Call ID: call_9X3chempPJwwuNs0hVeKMqe0\n",
      "  Args:\n",
      "    query: standard method for Task Decomposition\n",
      "  retrieve_context (call_IGVOpmzJplhII07BhKKyACXZ)\n",
      " Call ID: call_IGVOpmzJplhII07BhKKyACXZ\n",
      "  Args:\n",
      "    query: common extensions of the standard method for Task Decomposition\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}\n",
      "Content: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
      "Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into â€œProblem PDDLâ€, then (2) requests a classical planner to generate a PDDL plan based on an existing â€œDomain PDDLâ€, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\n",
      "Self-Reflection#\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}\n",
      "Content: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to â€œthink step by stepâ€ to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the modelâ€™s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The standard method for Task Decomposition is the \"Chain of Thought\" (CoT) prompting technique. CoT instructs the model to \"think step by step,\" decomposing hard tasks into smaller and simpler steps. This method transforms big tasks into multiple manageable tasks and provides an interpretation of the model's thinking process.\n",
      "\n",
      "Common extensions of the Chain of Thought method include:\n",
      "1. Tree of Thoughts (ToT): This extends CoT by exploring multiple reasoning possibilities at each step. The problem is decomposed into multiple thought steps, and multiple thoughts are generated per step, creating a tree structure. The search process can use breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or majority vote.\n",
      "2. LLM+P approach: This involves using an external classical planner for long-horizon planning, where the problem is translated into a Planning Domain Definition Language (PDDL) format, planned externally, and then translated back to natural language.\n",
      "\n",
      "Additionally, task decomposition can be done by simple LLM prompting, task-specific instructions, or with human inputs.\n"
     ]
    }
   ],
   "source": [
    "# åœ¨ LLM é‡Œï¼š\n",
    "# \\n\\n ä¼šè¢«æ¨¡å‹å¼ºçƒˆæ„ŸçŸ¥ä¸º â€œä»»åŠ¡åˆ‡æ¢ / æ–°é˜¶æ®µâ€\n",
    "# æ¯”å•çº¯ç”¨å¥å·æ›´åˆ©äºè§¦å‘ step-by-step è¡Œä¸º\n",
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    # åˆå§‹çŠ¶æ€\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    \n",
    "    # event[\"messages\"]\n",
    "    # è¿™æ˜¯ å½“å‰å¯¹è¯çŠ¶æ€ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œ\n",
    "    # [-1]ï¼šå½“å‰è¿™ä¸€æ­¥ agent æœ€æ–°äº§ç”Ÿçš„æ¶ˆæ¯\n",
    "\n",
    "    # .pretty_print() æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "    # è¿™æ˜¯ LangChain / LangGraph Message å¯¹è±¡è‡ªå¸¦çš„æ–¹æ³•ï¼Œç”¨æ¥ï¼š\n",
    "    # è‡ªåŠ¨åŒºåˆ†è§’è‰²ï¼ˆHuman / AI / Toolï¼‰\n",
    "    # ç¾åŒ–è¾“å‡º\n",
    "    # åœ¨ç»ˆç«¯é‡Œæ¸…æ™°æ˜¾ç¤º\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6a8e9",
   "metadata": {},
   "source": [
    "# RAG chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52311770",
   "metadata": {},
   "source": [
    "Another common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query.<br> \n",
    "\n",
    "This results in a single inference call per query, buying reduced latency at the expense of flexibility.<br>\n",
    "\n",
    "In this approach we no longer call the model in a loop, but instead make a single pass.<br>\n",
    "\n",
    "We can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050a3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "# ModelRequest å¯ä»¥ç†è§£ä¸ºï¼š\n",
    "# â€œä¸€æ¬¡å³å°†å‘é€ç»™ LLM çš„å®Œæ•´è¯·æ±‚å¯¹è±¡â€\n",
    "# é‡Œé¢é€šå¸¸åŒ…å«ï¼š\n",
    "#  request.stateï¼šAgent å½“å‰çš„çŠ¶æ€ï¼ˆmessagesã€memory ç­‰ï¼‰\n",
    "#  request.messagesï¼šæœ€ç»ˆè¦é€è¿›æ¨¡å‹çš„æ¶ˆæ¯\n",
    "# å…¶ä»–æ¨¡å‹è°ƒç”¨ç›¸å…³ä¿¡æ¯\n",
    "\n",
    "# â€œåœ¨æ¨¡å‹æ¯æ¬¡è¢«è°ƒç”¨å‰ï¼Œè¿è¡Œè¿™ä¸ªå‡½æ•°ï¼Œ\n",
    "# å¹¶æŠŠå®ƒçš„è¿”å›å€¼å½“ä½œ system prompt æ³¨å…¥è¿›å»ã€‚â€\n",
    "@dynamic_prompt\n",
    "def prompt_with_context(request: ModelRequest) -> str:\n",
    "    \"\"\"Inject context into state messages.\"\"\"\n",
    "    last_query = request.state[\"messages\"][-1].text\n",
    "    retrieved_docs = vector_store.similarity_search(last_query)\n",
    "\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "    # æŠŠ RAG å†…å®¹æ³¨å…¥ä¸º system messageï¼Œè€Œä¸æ˜¯ user message\n",
    "    # è¿™æ„å‘³ç€ï¼š\n",
    "    # æ¨¡å‹ä¼šæŠŠå®ƒå½“ä½œâ€œæœ€é«˜ä¼˜å…ˆçº§æŒ‡ä»¤â€\n",
    "    # ç”¨æˆ·æ— æ³•è½»æ˜“ override\n",
    "    # æ›´ç¨³å®šã€å¯æ§\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Use the following context in your response:\"\n",
    "        f\"\\n\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return system_message\n",
    "# æœ€åä¼šæŠŠ system prompt + å¯¹è¯ messages ä¸€èµ·é€ç»™æ¨¡å‹\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[], middleware=[prompt_with_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55f34d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is task decomposition?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Task decomposition is the process of breaking down a complex task or problem into smaller, more manageable sub-tasks or steps. This makes it easier to understand, plan, and execute the overall task by tackling each smaller part one at a time.\n",
      "\n",
      "Task decomposition can be done in several ways:\n",
      "\n",
      "1. Using Large Language Models (LLMs) with simple prompts, such as asking \"What are the steps for achieving XYZ?\" or \"What are the subgoals for completing XYZ?\"\n",
      "2. Employing task-specific instructions tailored to the domain, for example, instructing an LLM to \"Write a story outline\" when working on a novel.\n",
      "3. Incorporating human input to guide the breakdown based on expertise or experience.\n",
      "\n",
      "Additionally, there are more advanced approaches like combining LLMs with external classical planners (LLM+P). Here, the LLM translates the problem into a formal planning language (PDDL), an external planner generates a plan, and then the LLM converts that plan back into natural language. This method leverages specialized tools for long-horizon planning, especially in domains like robotics.\n",
      "\n",
      "In summary, task decomposition helps in organizing and simplifying complex tasks by dividing them into smaller, sequential or parallel steps, which can then be planned and executed more effectively.\n",
      "\n",
      "---\n",
      "\n",
      "Task Process and Analysis:\n",
      "\n",
      "User Input: \"What is task decomposition?\"\n",
      "\n",
      "Task Planning: The task was identified as an explanation request for the concept of task decomposition.\n",
      "\n",
      "Model Selection: I used a knowledge-based explanation model (myself as an LLM) to generate a clear and concise description.\n",
      "\n",
      "Task Execution: I provided a straightforward definition of task decomposition, outlined common methods to perform it, and mentioned advanced approaches like LLM+P for completeness.\n",
      "\n",
      "I hope this explanation clarifies what task decomposition is and how it is applied in AI and planning contexts. If you want, I can also provide examples or delve deeper into specific methods.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is task decomposition?\"\n",
    "for step in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a8c36",
   "metadata": {},
   "source": [
    "The above RAG chain incorporates retrieved context into a single system message for that run.<br>\n",
    "As in the agentic RAG formulation, we sometimes want to **include raw source documents in the application state** to have access to **document metadata**. We can do this for the two-step chain case by:\n",
    "1. Adding a key to the state to store the retrieved documents\n",
    "2. Adding a new node via a pre-model hook to populate that key (as well as inject the context)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd78a6",
   "metadata": {},
   "source": [
    "### 1ï¼‰Stateï¼šç»™ agent çš„çŠ¶æ€åŠ ä¸€ä¸ªå­—æ®µ context\n",
    "```python\n",
    "class State(AgentState):\n",
    "    context: list[Document]\n",
    "```\n",
    "\n",
    "- AgentState æ˜¯æ¡†æ¶é‡Œçº¦å®šçš„â€œçŠ¶æ€å­—å…¸â€åŸºç±»ï¼Œé€šå¸¸è‡³å°‘åŒ…å« messagesã€‚\n",
    "\n",
    "- ä½ è¿™é‡Œæ‰©å±•äº†ä¸€ä¸ªå­—æ®µï¼šcontextï¼Œç±»å‹æ˜¯ list[Document]ã€‚\n",
    "\n",
    "- Document æ˜¯ LangChain çš„æ–‡æ¡£å¯¹è±¡ï¼Œå¸¸è§å±æ€§ï¼š\n",
    "\n",
    "  -  page_contentï¼šæ­£æ–‡æ–‡æœ¬\n",
    "\n",
    "  - metadataï¼šæ¥æºã€é¡µç ã€URL ç­‰\n",
    "\n",
    "**ç›®çš„ï¼š** è®© agent åœ¨è¿è¡Œè¿‡ç¨‹ä¸­èƒ½æŠŠâ€œæ£€ç´¢åˆ°çš„æ–‡æ¡£â€ä½œä¸ºç»“æ„åŒ–æ•°æ®å­˜ä¸‹æ¥ï¼Œè€Œä¸ä»…ä»…æ˜¯æ‹¼åˆ° prompt é‡Œã€‚\n",
    "\n",
    "### 2ï¼‰RetrieveDocumentsMiddlewareï¼šä¸€ä¸ªâ€œæ¨¡å‹è°ƒç”¨å‰â€çš„ä¸­é—´ä»¶\n",
    "```python\n",
    "class RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n",
    "    state_schema = State\n",
    "```\n",
    "\n",
    "AgentMiddleware[State] è¡¨ç¤ºè¿™ä¸ªä¸­é—´ä»¶æ“ä½œçš„ state éµå¾ªä½ å®šä¹‰çš„ State ç»“æ„ï¼ˆå¸¦ contextï¼‰ã€‚\n",
    "\n",
    "state_schema = State æ˜¯å‘Šè¯‰æ¡†æ¶ï¼šè¿™ä¸ª middleware è¯»å†™ state æ—¶æŒ‰ State æ ¡éªŒ/æ¨æ–­å­—æ®µã€‚\n",
    "\n",
    "### 3ï¼‰before_modelï¼šå…³é”®é’©å­ï¼Œåœ¨æ¯æ¬¡ LLM è°ƒç”¨å‰è¿è¡Œ\n",
    "```python\n",
    "def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "```\n",
    "\n",
    "è¯­ä¹‰æ˜¯ï¼š\n",
    "\n",
    "æ¯å½“ agent å‡†å¤‡è°ƒç”¨æ¨¡å‹ï¼ˆLLMï¼‰ä¹‹å‰ï¼Œå…ˆæŠŠå½“å‰ state äº¤ç»™ä½ ï¼›\n",
    "ä½ å¯ä»¥è¿”å›ä¸€ä¸ª dict æ¥â€œè¦†ç›–/è¿½åŠ â€æœ¬æ¬¡æ¨¡å‹è°ƒç”¨ç”¨åˆ°çš„è¾“å…¥ï¼ˆä»¥åŠæ›´æ–° stateï¼‰ï¼Œä¹Ÿå¯ä»¥è¿”å› None è¡¨ç¤ºä¸æ”¹ã€‚\n",
    "\n",
    "æ³¨æ„ï¼šä¸åŒç‰ˆæœ¬å®ç°ç»†èŠ‚ç•¥æœ‰ä¸åŒï¼Œä½†è¿™ä¸ªé’©å­çš„ç›®æ ‡å°±æ˜¯ **â€œæ”¹è¾“å…¥ã€æ”¹ stateâ€ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebb396b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.documents import Document\n",
    "from langchain.agents.middleware import AgentMiddleware, AgentState\n",
    "\n",
    "\n",
    "class State(AgentState):\n",
    "    context: list[Document]\n",
    "\n",
    "\n",
    "class RetrieveDocumentsMiddleware(AgentMiddleware[State]):\n",
    "    state_schema = State\n",
    "\n",
    "    def before_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        retrieved_docs = vector_store.similarity_search(last_message.text)\n",
    "\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "        augmented_message_content = (\n",
    "            f\"{last_message.text}\\n\\n\"\n",
    "            \"Use the following context to answer the query:\\n\"\n",
    "            f\"{docs_content}\"\n",
    "        )\n",
    "        return {\n",
    "            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\n",
    "            \"context\": retrieved_docs,\n",
    "        }\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model,\n",
    "    tools=[],\n",
    "    middleware=[RetrieveDocumentsMiddleware()],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
